# -*- coding: utf-8 -*-
"""Homework_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103cO0vcfEMJn6fq6EcA2NW3Jf483tLN4
"""

pip install textblob

import pandas as pd
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC, LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, precision_recall_curve, average_precision_score, f1_score, auc
from nltk.tokenize import word_tokenize
from collections import Counter
from sklearn import preprocessing
from sklearn.model_selection import RandomizedSearchCV
from sklearn.multiclass import OneVsRestClassifier
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import label_binarize
from itertools import cycle

dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP 220/ecommerceDataset.csv')
dataset.columns = ['Category', 'Description']
dataset.head()

from google.colab import drive
drive.mount('/content/drive')

dataset.shape

print(dataset.dtypes)

# Feature 1
dataset['Description'] = dataset['Description'].fillna('')
dataset['Description_Length'] = dataset['Description'].apply(lambda x: len(x.split()))
dataset.head()

# Feature 2
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

dataset['Sentiment'] = dataset['Description'].apply(get_sentiment)
dataset.head()

# Feature 3
nltk.download('stopwords')
stop_words = list(set(stopwords.words('english')))

vectorizer = CountVectorizer(max_features=10, stop_words=stop_words)
X = vectorizer.fit_transform(dataset['Description'])
keywords = vectorizer.get_feature_names_out()
dataset['Keywords'] = dataset['Description'].apply(
    lambda x: [word for word in keywords if word in x.lower()])

dataset.head()

# Feature 4 - Bag of Words
vectorizer = CountVectorizer(max_features=1000)
word_freq_matrix = vectorizer.fit_transform(dataset['Description'])
word_freq_df = pd.DataFrame(word_freq_matrix.toarray(), columns=vectorizer.get_feature_names_out())
word_freq_df.head()

tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(dataset['Description'])
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),
                        columns=tfidf_vectorizer.get_feature_names_out())
tfidf_df.head()

dataset['Category'].unique()
dataset['Label'] = dataset['Category'].apply(lambda x: 0 if x == 'Household'
                                             else 1 if x == 'Books'
                                             else 2 if x == 'Clothing & Accessories'
                                             else 3 if x == 'Electronics'
                                             else None)
dataset.head()

label_counts = dataset['Label'].value_counts()
label_percentages = dataset['Label'].value_counts(normalize=True) * 100

print(label_counts)
print(label_percentages)

plt.figure(figsize=(8, 6))
bars = plt.bar(label_counts.index, label_counts.values, color='skyblue')
plt.title('Distribution of Classes/Labels')
plt.xlabel('Label')
plt.ylabel('Count')
plt.xticks(label_counts.index)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval, int(yval), ha='center', va='bottom')

plt.show()

# Feature Engineering 1
X_f1 = pd.concat([dataset[['Description_Length', 'Sentiment']], word_freq_df, tfidf_df], axis=1)
y = dataset['Label']

# Split into train and test set
X_train1, X_temp1, y_train1, y_temp1 = train_test_split(X_f1, y, test_size=0.3, random_state=42, shuffle=True)

# Split remaining into validation and test set
X_val1, X_test1, y_val1, y_test1 = train_test_split(X_temp1, y_temp1, test_size=2/3, random_state=42, shuffle=True)

# sc = StandardScaler()

# X_train1 = sc.fit_transform(X_train1)
# X_test1 = sc.transform(X_test1)

# SVM - 1
classifierSVM1 = LinearSVC(random_state=42)
classifierSVM1.fit(X_train1, y_train1)
y_pred1 = classifierSVM1.predict(X_test1)

# Evaluate the model
accuracy = accuracy_score(y_test1, y_pred1)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test1, y_pred1))
print(confusion_matrix(y_test1, y_pred1))

# Decision Tree - 1
classifierDT1 = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)
classifierDT1.fit(X_train1, y_train1)

y_pred1 = classifierDT1.predict(X_test1)

# Evaluate the model
accuracy = accuracy_score(y_test1, y_pred1)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test1, y_pred1))
print(confusion_matrix(y_test1, y_pred1))

sc = preprocessing.StandardScaler()
X_train1 = sc.fit_transform(X_train1)
X_test1 = sc.transform(X_test1)

# Logistic Regression - 1
classifierLR1 = LogisticRegression(random_state = 42)
classifierLR1.fit(X_train1, y_train1)

y_pred1 = classifierLR1.predict(X_test1)

# Evaluate the model
accuracy = accuracy_score(y_test1, y_pred1)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test1, y_pred1, zero_division='warn'))
print(confusion_matrix(y_test1, y_pred1))

# Feature Engineering 2
X_f2 = pd.concat([dataset[['Description_Length', 'Sentiment']], tfidf_df], axis=1)
y = dataset['Label']

# Split into train and test set
X_train2, X_temp2, y_train2, y_temp2 = train_test_split(X_f2, y, test_size=0.3, random_state=42, shuffle=True)

# Split remaining into validation and test set
X_val2, X_test2, y_val2, y_test2 = train_test_split(X_temp2, y_temp2, test_size=2/3, random_state=42, shuffle=True)

# sc = StandardScaler()

# X_train2 = sc.fit_transform(X_train1)
# X_test2 = sc.transform(X_test1)

# SVM - 2
classifierSVM2 = LinearSVC(random_state=42)
classifierSVM2.fit(X_train2, y_train2)
y_pred2 = classifierSVM2.predict(X_test2)

# Evaluate the model
accuracy = accuracy_score(y_test2, y_pred2)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test2, y_pred2))
print(confusion_matrix(y_test2, y_pred2))

# Decision Tree - 2
classifierDT2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)
classifierDT2.fit(X_train2, y_train2)

y_pred2 = classifierDT2.predict(X_test2)

# Evaluate the model
accuracy = accuracy_score(y_test2, y_pred2)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test2, y_pred2))
print(confusion_matrix(y_test2, y_pred2))

sc = preprocessing.StandardScaler()

X_train2 = sc.fit_transform(X_train2)
X_test2 = sc.transform(X_test2)

# Logistic Regression - 2
classifierLR2 = LogisticRegression(random_state = 42)
classifierLR2.fit(X_train2, y_train2)

y_pred2 = classifierLR2.predict(X_test2)

# Evaluate the model
accuracy = accuracy_score(y_test2, y_pred2)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test2, y_pred2))
print(confusion_matrix(y_test2, y_pred2))

# Feature Engineering 3
X_f3 = pd.concat([dataset[['Description_Length', 'Sentiment']], word_freq_df], axis=1)
y = dataset['Label']

# Split into train and test set
X_train3, X_temp3, y_train3, y_temp3 = train_test_split(X_f3, y, test_size=0.3, random_state=42, shuffle=True)

# Split remaining into validation and test set
X_val3, X_test3, y_val3, y_test3 = train_test_split(X_temp3, y_temp3, test_size=2/3, random_state=42, shuffle=True)

# sc = StandardScaler()

# X_train2 = sc.fit_transform(X_train1)
# X_test2 = sc.transform(X_test1)

# SVM - 3
classifierSVM3 = LinearSVC(random_state=42)
classifierSVM3.fit(X_train3, y_train3)
y_pred3 = classifierSVM3.predict(X_test3)

# Evaluate the model
accuracy = accuracy_score(y_test3, y_pred3)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test3, y_pred3))
print(confusion_matrix(y_test3, y_pred3))

# Decision Tree - 3
classifierDT3 = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)
classifierDT3.fit(X_train3, y_train3)

y_pred3 = classifierDT3.predict(X_test3)

# Evaluate the model
accuracy = accuracy_score(y_test3, y_pred3)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test3, y_pred3))
print(confusion_matrix(y_test3, y_pred3))

sc = preprocessing.StandardScaler()
X_train3 = sc.fit_transform(X_train3)
X_test3 = sc.transform(X_test3)

# Logistic Regression - 3
classifierLR3 = LogisticRegression(random_state = 42)
classifierLR3.fit(X_train3, y_train3)

y_pred3 = classifierLR3.predict(X_test3)

# Evaluate the model
accuracy = accuracy_score(y_test3, y_pred3)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test3, y_pred3))
print(confusion_matrix(y_test3, y_pred3))

# Simplified parameter space
param_distributions = {
    'C': [0.1, 1.0, 10.0, 100.0],  # Only 3 values for C
    'loss': ['squared_hinge'],  # Only default loss function
    'tol': [1e-3, 1e-4],  # Only one tolerance value
    'max_iter': [1500],
    'dual': [True],
    'class_weight': ['balanced']
}

svc = LinearSVC(random_state=42)

# Initialize RandomizedSearchCV with minimal iterations
random_searchSVC = RandomizedSearchCV(
    estimator=svc,
    param_distributions=param_distributions,
    n_iter=5,
    cv=2,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# Fit the model
random_searchSVC.fit(X_train1, y_train1)

# Print the best parameters and score
print("Best parameters:", random_searchSVC.best_params_)
print("Best cross-validation score:", random_searchSVC.best_score_)

# Make predictions on validation set
y_pred_val = random_searchSVC.predict(X_val1)
print("\nValidation Set Performance:")
print("Accuracy:", accuracy_score(y_val1, y_pred_val))
print("\nClassification Report:")
print(classification_report(y_val1, y_pred_val))

# Save the best model
best_model = random_searchSVC.best_estimator_

# Make predictions on test set
y_pred_test = best_model.predict(X_test1)
print("\nTest Set Performance:")
print("Accuracy:", accuracy_score(y_test1, y_pred_test))
print("\nClassification Report:")
print(classification_report(y_test1, y_pred_test))

# Simplified parameter space
param_distributions = {
    'max_depth': [3, 5, 7, 10, None],
    'min_samples_split': [2, 4, 6, 8],
    'min_samples_leaf': [1, 2, 3, 4],
    'criterion': ['gini', 'entropy','log_loss'],
    'splitter': ['best', 'random'],
    'max_features': ['auto', 'sqrt', 'log2']
}

dt = DecisionTreeClassifier(random_state=42)

# Initialize RandomizedSearchCV with minimal iterations
random_searchDT = RandomizedSearchCV(
    estimator=dt,
    param_distributions=param_distributions,
    n_iter=5,
    cv=2,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# Fit the model
random_searchDT.fit(X_train1, y_train1)

# Print the best parameters and score
print("Best parameters:", random_searchDT.best_params_)
print("Best cross-validation score:", random_searchDT.best_score_)

# Make predictions on validation set
y_pred_val = random_searchDT.predict(X_val1)
print("\nValidation Set Performance:")
print("Accuracy:", accuracy_score(y_val1, y_pred_val))
print("\nClassification Report:")
print(classification_report(y_val1, y_pred_val))

# Save the best model
best_model = random_searchDT.best_estimator_

# Make predictions on test set
y_pred_test = best_model.predict(X_test1)
print("\nTest Set Performance:")
print("Accuracy:", accuracy_score(y_test1, y_pred_test))
print("\nClassification Report:")
print(classification_report(y_test1, y_pred_test))

# Simplified parameter space
param_distributions = {
    'C': [0.1, 1.0, 10.0],
    'solver': ['lbfgs'],  # Using only one solver for speed
    'max_iter': [1000]
}

lr = LogisticRegression(random_state=42)

# Initialize RandomizedSearchCV with minimal iterations
random_searchLR = RandomizedSearchCV(
    estimator=lr,
    param_distributions=param_distributions,
    n_iter=5,
    cv=2,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

# Fit the model
random_searchLR.fit(X_train1, y_train1)

# Print the best parameters and score
print("Best parameters:", random_searchLR.best_params_)
print("Best cross-validation score:", random_searchLR.best_score_)

# Make predictions on validation set
y_pred_val = random_searchLR.predict(X_val1)
print("\nValidation Set Performance:")
print("Accuracy:", accuracy_score(y_val1, y_pred_val))
print("\nClassification Report:")
print(classification_report(y_val1, y_pred_val))

# Save the best model
best_model = random_searchLR.best_estimator_

# Make predictions on test set
y_pred_test = best_model.predict(X_test1)
print("\nTest Set Performance:")
print("Accuracy:", accuracy_score(y_test1, y_pred_test))
print("\nClassification Report:")
print(classification_report(y_test1, y_pred_test))

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, f1_score
from sklearn.multiclass import OneVsRestClassifier
import matplotlib.pyplot as plt

# Initialize the base classifier with Logistic Regression
base_classifier = LogisticRegression(random_state=42)

# Create OneVsRestClassifier
ovr_classifier = OneVsRestClassifier(base_classifier)

# Fit the model
ovr_classifier.fit(X_train1, y_train1)

# Get predictions and probability scores
y_pred = ovr_classifier.predict(X_test1)
y_prob = ovr_classifier.predict_proba(X_test1)

# Convert test labels to binary format
classes = sorted(list(set(y_test1)))
y_test_bin = label_binarize(y_test1, classes=classes)

# Calculate individual F1 scores and average
f1_scores = []
for i in range(len(classes)):
    f1 = f1_score(y_test_bin[:, i], y_prob[:, i] > 0.5)
    f1_scores.append(f1)
    print(f"F1 score for class {classes[i]}: {f1:.3f}")

avg_f1 = np.mean(f1_scores)
print(f"\nAverage Macro F1 score: {avg_f1:.3f}")

# Plot ROC curves
plt.figure(figsize=(10, 6))
colors = ['blue', 'red', 'green', 'purple']
for i, color in zip(range(len(classes)), colors):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=color, lw=2,
             label=f'ROC class {classes[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - One vs Rest')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall curves
plt.figure(figsize=(10, 6))
for i, color in zip(range(len(classes)), colors):
    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_prob[:, i])
    avg_precision = average_precision_score(y_test_bin[:, i], y_prob[:, i])
    plt.plot(recall, precision, color=color, lw=2,
             label=f'PR class {classes[i]} (AP = {avg_precision:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves - One vs Rest')
plt.legend(loc="lower left")
plt.show()

# Print summary statistics
print("\nSummary of OneVsRest Classification:")
print("-" * 40)
print(f"Number of classes: {len(classes)}")
print(f"Classes: {classes}")
print("\nIndividual class F1 scores:")
for i, score in enumerate(f1_scores):
    print(f"Class {classes[i]}: {score:.3f}")
print(f"\nMacro-averaged F1 score: {avg_f1:.3f}")

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, f1_score
from sklearn.multiclass import OneVsRestClassifier
import matplotlib.pyplot as plt

base_classifier = LogisticRegression(random_state=42)
ovr_classifier = OneVsRestClassifier(base_classifier)

ovr_classifier.fit(X_train1, y_train1)

# Get predictions and probability scores
y_pred = ovr_classifier.predict(X_test1)
y_prob = ovr_classifier.predict_proba(X_test1)

# Convert test labels to binary format
classes = sorted(list(set(y_test1)))
y_test_bin = label_binarize(y_test1, classes=classes)

# Calculate individual F1 scores and average
f1_scores = []
for i in range(len(classes)):
    f1 = f1_score(y_test_bin[:, i], y_prob[:, i] > 0.5)
    f1_scores.append(f1)
    print(f"F1 score for class {classes[i]}: {f1:.3f}")

avg_f1 = np.mean(f1_scores)
print(f"\nAverage Macro F1 score: {avg_f1:.3f}")

colors = ['blue', 'red', 'green', 'purple']

# Plot individual ROC curves
for i, (color, class_name) in enumerate(zip(colors, classes)):
    plt.figure(figsize=(8, 6))

    # Calculate ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)

    # Plot ROC curve
    plt.plot(fpr, tpr, color=color, lw=2,
             label=f'AUC = {roc_auc:.2f}')
    plt.plot([0, 1], [0, 1], 'k--', lw=2)

    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - Class {class_name}')
    plt.legend(loc="lower right")
    plt.show()

# Plot individual Precision-Recall curves
for i, (color, class_name) in enumerate(zip(colors, classes)):
    plt.figure(figsize=(8, 6))

    # Calculate Precision-Recall curve and average precision
    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_prob[:, i])
    avg_precision = average_precision_score(y_test_bin[:, i], y_prob[:, i])

    # Plot Precision-Recall curve
    plt.plot(recall, precision, color=color, lw=2,
             label=f'AP = {avg_precision:.2f}')

    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - Class {class_name}')
    plt.legend(loc="lower left")
    plt.show()

# Print summary statistics
print("\nSummary of OneVsRest Classification:")
print("-" * 40)
print(f"Number of classes: {len(classes)}")
print(f"Classes: {classes}")
print("\nIndividual class F1 scores:")
for i, score in enumerate(f1_scores):
    print(f"Class {classes[i]}: {score:.3f}")
print(f"\nMacro-averaged F1 score: {avg_f1:.3f}")

# Print detailed metrics for each class
print("\nDetailed Metrics for Each Class:")
print("-" * 40)
for i, class_name in enumerate(classes):
    print(f"\nClass {class_name}:")
    print(f"F1 Score: {f1_scores[i]:.3f}")
    avg_precision = average_precision_score(y_test_bin[:, i], y_prob[:, i])
    print(f"Average Precision: {avg_precision:.3f}")
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    print(f"AUC-ROC: {roc_auc:.3f}")

# Initialize the base classifier
base_classifier = SVC(kernel='linear', probability=True, random_state=42)

# Create OneVsRestClassifier
ovr_classifier = OneVsRestClassifier(base_classifier)

# Fit the model
ovr_classifier.fit(X_train1, y_train1)

# Get predictions and probability scores
y_pred = ovr_classifier.predict(X_test1)
y_prob = ovr_classifier.predict_proba(X_test1)

# Convert test labels to binary format
classes = sorted(list(set(y_test1)))
y_test_bin = label_binarize(y_test1, classes=classes)

# Calculate individual F1 scores and average
f1_scores = []
for i in range(len(classes)):
    f1 = f1_score(y_test_bin[:, i], y_prob[:, i] > 0.5)
    f1_scores.append(f1)
    print(f"F1 score for class {classes[i]}: {f1:.3f}")

avg_f1 = np.mean(f1_scores)
print(f"\nAverage Macro F1 score: {avg_f1:.3f}")

# Plot ROC curves
plt.figure(figsize=(10, 6))
colors = ['blue', 'red', 'green', 'purple']
for i, color in zip(range(len(classes)), colors):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, color=color, lw=2,
             label=f'ROC class {classes[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves - One vs Rest')
plt.legend(loc="lower right")
plt.show()

# Plot Precision-Recall curves
plt.figure(figsize=(10, 6))
for i, color in zip(range(len(classes)), colors):
    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_prob[:, i])
    avg_precision = average_precision_score(y_test_bin[:, i], y_prob[:, i])
    plt.plot(recall, precision, color=color, lw=2,
             label=f'PR class {classes[i]} (AP = {avg_precision:.2f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curves - One vs Rest')
plt.legend(loc="lower left")
plt.show()

# Print summary statistics
print("\nSummary of OneVsRest Classification:")
print("-" * 40)
print(f"Number of classes: {len(classes)}")
print(f"Classes: {classes}")
print("\nIndividual class F1 scores:")
for i, score in enumerate(f1_scores):
    print(f"Class {classes[i]}: {score:.3f}")
print(f"\nMacro-averaged F1 score: {avg_f1:.3f}")

# Get predictions
y_pred = clf.predict(X_test1)

# Convert binary predictions back to multiclass
y_pred_multiclass = np.argmax(y_pred, axis=1)
y_test_multiclass = np.argmax(y_test_bin, axis=1)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_multiclass, y_pred_multiclass)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print class distribution
print("\nClass Distribution in Test Set:")
for i in range(n_classes):
    class_count = np.sum(y_test_multiclass == i)
    print(f"Class {i}: {class_count} samples ({class_count/len(y_test_multiclass)*100:.2f}%)")

# Print detailed classification report
print("\nDetailed Classification Report:")
print(classification_report(y_test_multiclass, y_pred_multiclass))

# Check feature importance (for interpretability)
if hasattr(base_classifier, 'coef_'):
    # Get feature importance for each class
    feature_importance = np.abs(clf.estimators_[0].coef_)
    # Get top 10 features for first class
    feature_names = np.arange(X_test1.shape[1])  # Replace with actual feature names if available
    top_features = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance[0]
    }).sort_values('Importance', ascending=False).head(10)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=top_features, x='Importance', y='Feature')
    plt.title('Top 10 Most Important Features (Class 0)')
    plt.show()

# Calculate prediction probabilities distribution
decision_scores = clf.decision_function(X_test1)
plt.figure(figsize=(12, 6))
for i in range(n_classes):
    plt.hist(decision_scores[:, i], bins=50, alpha=0.5, label=f'Class {i}')
plt.title('Distribution of Decision Scores')
plt.xlabel('Decision Score')
plt.ylabel('Count')
plt.legend()
plt.show()

# Get predictions
y_pred = clf.predict(X_test1)

# Convert binary predictions back to multiclass
y_pred_multiclass = np.argmax(y_pred, axis=1)
y_test_multiclass = np.argmax(y_test_bin, axis=1)

# Plot confusion matrix
plt.figure(figsize=(10, 8))
cm = confusion_matrix(y_test_multiclass, y_pred_multiclass)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print class distribution
print("\nClass Distribution in Test Set:")
for i in range(n_classes):
    class_count = np.sum(y_test_multiclass == i)
    print(f"Class {i}: {class_count} samples ({class_count/len(y_test_multiclass)*100:.2f}%)")

# Print detailed classification report
print("\nDetailed Classification Report:")
print(classification_report(y_test_multiclass, y_pred_multiclass))

# Check feature importance (for interpretability)
if hasattr(base_classifier, 'coef_'):
    # Get feature importance for each class
    feature_importance = np.abs(clf.estimators_[0].coef_)
    # Get top 10 features for first class
    feature_names = np.arange(X_test1.shape[1])  # Replace with actual feature names if available
    top_features = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importance[0]
    }).sort_values('Importance', ascending=False).head(10)

    plt.figure(figsize=(10, 6))
    sns.barplot(data=top_features, x='Importance', y='Feature')
    plt.title('Top 10 Most Important Features (Class 0)')
    plt.show()

# Calculate prediction probabilities distribution
decision_scores = clf.decision_function(X_test1)
plt.figure(figsize=(12, 6))
for i in range(n_classes):
    plt.hist(decision_scores[:, i], bins=50, alpha=0.5, label=f'Class {i}')
plt.title('Distribution of Decision Scores')
plt.xlabel('Decision Score')
plt.ylabel('Count')
plt.legend()
plt.show()